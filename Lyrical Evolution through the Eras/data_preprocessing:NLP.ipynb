{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Correctly classify Taylor Swift lyric to correct album based on text data alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation packages\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# textual analysis packages \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in dataset\n",
    "data = pd.read_csv('thisone.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/AkFSOvF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Engineering\n",
    "Before I can use my data in a machine learning model, I first have to transform my lyrical text features to numerical features for the machine to read the data. To do so, I will be using TF-IDF vectorization. <br>\n",
    "<br>\n",
    "I am choosing to use tf-idf vectorization based on my knowledge of her lyrics as well as\n",
    "what I observed in my data exploration, because I need to use something that will minimize the similaries observed throughout the catalog while maximizing the uniqueness of each album.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop Words**\n",
    "- The stop words that come preloaded with sk learn are not appropriate for my datatset. They contain words like \"mine\", \"ours\", and \"fifteen\" which are important 'words' in Taylor Swift's lyrics. Because of this, I will be creating my own custom list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4584, 3907)\n"
     ]
    }
   ],
   "source": [
    "# stop words\n",
    "stop_words = ['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', \n",
    "              'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with']\n",
    "\n",
    "# isolating the textual data\n",
    "lyric_text = data['lyrics']\n",
    "\n",
    "# creating tf-idf object, removing stop words as well as stripping ascii characters.\n",
    "vectorizer = TfidfVectorizer(strip_accents = 'ascii', stop_words = stop_words)\n",
    "\n",
    "# fitting and transforming the text data\n",
    "X = vectorizer.fit_transform(lyric_text)\n",
    "\n",
    "# checking shape\n",
    "print(X.shape)\n",
    "\n",
    "# getting feature names for further analysis\n",
    "feature_names = vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4584x3907 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 40591 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting more information about my matrix\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shape**\n",
    "- This shape (4584, 3906) shows that I have 4584 data points and a feature space of 3906. This means that I have 3906 unique features in my dataset.\n",
    "\n",
    "\n",
    "**Matrix**\n",
    "- Tf-Idf vectorization creates a sparse matrix, which means that most of the values within the matrix are 0. The sparsity of the matrix arises from the nature of TF-IDF vectorization. In a TF-IDF matrix, each row corresponds to a document, or in my case, a lyrical snippet, and each column corresponds to a unique feature. Since most documents/lyrics only contain a handful of those features, most of the features for the given data point will be 0.\n",
    "- To explain this further, I will look closer at my first datapoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3907 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this show that out of the 3700 features, my first datapoint only has 13 with a TF-IDF value greater than 0\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2242)\t0.21791958139285975\n",
      "  (0, 2918)\t0.3133642375920805\n",
      "  (0, 3197)\t0.3133642375920805\n",
      "  (0, 1390)\t0.42213123800187724\n",
      "  (0, 3437)\t0.2897126203042577\n",
      "  (0, 2597)\t0.2666172045126449\n",
      "  (0, 2938)\t0.3551549893266243\n",
      "  (0, 1145)\t0.2376855262647906\n",
      "  (0, 343)\t0.2897126203042577\n",
      "  (0, 2194)\t0.13630087493371815\n",
      "  (0, 3729)\t0.2312342039416239\n",
      "  (0, 2811)\t0.21102944450978636\n",
      "  (0, 1560)\t0.19939999199169645\n",
      "He said the way my blue eyes shined Put those Georgia stars to shame that night\n"
     ]
    }
   ],
   "source": [
    "# those features are:\n",
    "print(X[0])\n",
    "\n",
    "# which are TF-IDF representations of the words from the lyric from the first data point:\n",
    "print(data['lyrics'][0])\n",
    "\n",
    "# the reason why there are only 13 non-zero features comes from our removal of stop words, we removed 'the' and 'that'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that I have applied TF-IDF vectorization to my text data, I want to be able to look at the most important features in the dataset. This will not only allow me to visualize the top features, but I will also be able to remove unnecessary features that do not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that will allow me to visualize the top features\n",
    "def looking_at_top(X, features):\n",
    "    '''\n",
    "    Takes in vectorized data and feature names to show the top 20 most \n",
    "    important features\n",
    "    Inputs:\n",
    "        X: (numpy array) an array of tf-idf vectorized features\n",
    "        features: (numpy array) an array of the names of the features\n",
    "    '''\n",
    "    \n",
    "    feature_np = np.asarray(np.sum(X, axis = 0))\n",
    "    feature_np = feature_np.reshape(-1)\n",
    "\n",
    "    top = []\n",
    "\n",
    "    for x in np.argsort(feature_np)[::-1][:21]:\n",
    "        top.append((features[x], feature_np[x]))\n",
    "\n",
    "        df = pd.DataFrame(top)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>325.207925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me</td>\n",
       "      <td>162.340023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my</td>\n",
       "      <td>134.966603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh</td>\n",
       "      <td>119.146428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>your</td>\n",
       "      <td>110.488174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>we</td>\n",
       "      <td>110.443744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all</td>\n",
       "      <td>103.570040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>97.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>know</td>\n",
       "      <td>93.512426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>so</td>\n",
       "      <td>80.275180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>don</td>\n",
       "      <td>79.734377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>when</td>\n",
       "      <td>79.637681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>re</td>\n",
       "      <td>77.753199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>just</td>\n",
       "      <td>74.409167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>can</td>\n",
       "      <td>71.868812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>never</td>\n",
       "      <td>71.578676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>love</td>\n",
       "      <td>69.880959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>now</td>\n",
       "      <td>67.807400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>what</td>\n",
       "      <td>64.094566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>time</td>\n",
       "      <td>62.779352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>up</td>\n",
       "      <td>61.317038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1\n",
       "0     you  325.207925\n",
       "1      me  162.340023\n",
       "2      my  134.966603\n",
       "3      oh  119.146428\n",
       "4    your  110.488174\n",
       "5      we  110.443744\n",
       "6     all  103.570040\n",
       "7    like   97.357143\n",
       "8    know   93.512426\n",
       "9      so   80.275180\n",
       "10    don   79.734377\n",
       "11   when   79.637681\n",
       "12     re   77.753199\n",
       "13   just   74.409167\n",
       "14    can   71.868812\n",
       "15  never   71.578676\n",
       "16   love   69.880959\n",
       "17    now   67.807400\n",
       "18   what   64.094566\n",
       "19   time   62.779352\n",
       "20     up   61.317038"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "looking_at_top(X, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From this we can see a lot more information about what features and important to the dataset. \n",
    "- It seems as though when I applied ascii stripping in the vectorizaiton, some of the words got cut off. Those probably need to be removed.\n",
    "- However, I need to look closely at words before I remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'re' can be found: (array([2637]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([\"So go and tell your friends that I'm obsessive and crazy That's fine I'll tell mine that you're gay!\",\n",
       "       \"You never let me drive You're a redneck heartbreak\"], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding where the feature is located\n",
    "print(\"\\'re\\' can be found:\", np.where(feature_names == 're'))\n",
    "\n",
    "# and looking at two of the text representations of that feature\n",
    "np.array(lyric_text)[np.where(X[:, 2637].toarray()>0)[0][:2]]\n",
    "\n",
    "# from here, we can see that it does indeed come from a contraction that was stripped during ascii.\n",
    "# I will be adding this feature to my stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\don' can be found: (array([963]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([\"He's the song in the car I keep singing. Don't know why I do Drew walks by me\",\n",
       "       \"The only thing that keeps me wishing on a wishing star He's the song in the car I keep singing. Don't know why I do\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeating for the other words\n",
    "\n",
    "# finding where the feature is located\n",
    "print(\"\\don\\' can be found:\", np.where(feature_names == 'don'))\n",
    "# and looking at two of the text representations of that feature\n",
    "np.array(lyric_text)[np.where(X[:, 963].toarray()>0)[0][:2]]\n",
    "# same thing happend here, adding to stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4584, 3905)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>328.488988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me</td>\n",
       "      <td>163.383899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my</td>\n",
       "      <td>135.400737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh</td>\n",
       "      <td>119.578644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we</td>\n",
       "      <td>111.157969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>your</td>\n",
       "      <td>110.884543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all</td>\n",
       "      <td>103.901335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>97.841402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>know</td>\n",
       "      <td>94.489719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>so</td>\n",
       "      <td>80.845722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>when</td>\n",
       "      <td>80.050428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>just</td>\n",
       "      <td>74.853317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>can</td>\n",
       "      <td>72.219255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>never</td>\n",
       "      <td>71.777691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>love</td>\n",
       "      <td>70.400290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>now</td>\n",
       "      <td>68.349691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>what</td>\n",
       "      <td>64.493544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>time</td>\n",
       "      <td>63.080658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>up</td>\n",
       "      <td>61.570357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ll</td>\n",
       "      <td>60.000942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ve</td>\n",
       "      <td>59.387282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1\n",
       "0     you  328.488988\n",
       "1      me  163.383899\n",
       "2      my  135.400737\n",
       "3      oh  119.578644\n",
       "4      we  111.157969\n",
       "5    your  110.884543\n",
       "6     all  103.901335\n",
       "7    like   97.841402\n",
       "8    know   94.489719\n",
       "9      so   80.845722\n",
       "10   when   80.050428\n",
       "11   just   74.853317\n",
       "12    can   72.219255\n",
       "13  never   71.777691\n",
       "14   love   70.400290\n",
       "15    now   68.349691\n",
       "16   what   64.493544\n",
       "17   time   63.080658\n",
       "18     up   61.570357\n",
       "19     ll   60.000942\n",
       "20     ve   59.387282"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words.extend(['re', 'don'])\n",
    "\n",
    "# since I have added to stop words, I will now rerun the TF-IDF vectorization with the new stop words and then run the top features function to see if the top words make sense\n",
    "vectorizer = TfidfVectorizer(strip_accents = 'ascii', stop_words = stop_words)\n",
    "\n",
    "# fitting and transforming the text data\n",
    "X = vectorizer.fit_transform(lyric_text)\n",
    "\n",
    "# checking shape\n",
    "print(X.shape)\n",
    "\n",
    "# getting feature names for further analysis\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# checking top features\n",
    "looking_at_top(X, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ve' can be found: (array([3641]),)\n",
      "'ll' can be found: (array([1953]),)\n"
     ]
    }
   ],
   "source": [
    "# there are still words that do not make sense, so I will be repeating my process again\n",
    "# finding where the feature is located\n",
    "print(\"\\'ve\\' can be found:\", np.where(feature_names == 've'))\n",
    "# looking at two of the text representations of that feature\n",
    "np.array(lyric_text)[np.where(X[:, 3461].toarray()>0)[0][:2]]\n",
    "\n",
    "# finding where the feature is located\n",
    "print(\"\\'ll\\' can be found:\", np.where(feature_names == 'll'))\n",
    "# looking at two of the text representations of that feature\n",
    "np.array(lyric_text)[np.where(X[:, 1859].toarray()>0)[0][:2]]\n",
    "\n",
    "# extending the list\n",
    "stop_words.extend(['ve', 'll', 'isn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4584, 3902)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>330.548768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me</td>\n",
       "      <td>164.067990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my</td>\n",
       "      <td>136.058706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh</td>\n",
       "      <td>119.832002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we</td>\n",
       "      <td>111.687749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>your</td>\n",
       "      <td>111.220208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all</td>\n",
       "      <td>104.576143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>97.988772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>know</td>\n",
       "      <td>95.218517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>so</td>\n",
       "      <td>81.211411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>when</td>\n",
       "      <td>80.277854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>just</td>\n",
       "      <td>75.262839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>can</td>\n",
       "      <td>72.338164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>never</td>\n",
       "      <td>72.258929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>love</td>\n",
       "      <td>70.750755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>now</td>\n",
       "      <td>68.624451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>what</td>\n",
       "      <td>64.830135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>time</td>\n",
       "      <td>63.291720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>up</td>\n",
       "      <td>61.756065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cause</td>\n",
       "      <td>59.926288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>back</td>\n",
       "      <td>58.062381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1\n",
       "0     you  330.548768\n",
       "1      me  164.067990\n",
       "2      my  136.058706\n",
       "3      oh  119.832002\n",
       "4      we  111.687749\n",
       "5    your  111.220208\n",
       "6     all  104.576143\n",
       "7    like   97.988772\n",
       "8    know   95.218517\n",
       "9      so   81.211411\n",
       "10   when   80.277854\n",
       "11   just   75.262839\n",
       "12    can   72.338164\n",
       "13  never   72.258929\n",
       "14   love   70.750755\n",
       "15    now   68.624451\n",
       "16   what   64.830135\n",
       "17   time   63.291720\n",
       "18     up   61.756065\n",
       "19  cause   59.926288\n",
       "20   back   58.062381"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since I have added to stop words, I will now rerun the TF-IDF vectorization with the new stop words and then run the top features function to see if the top words make sense\n",
    "vectorizer = TfidfVectorizer(strip_accents = 'ascii', stop_words = stop_words)\n",
    "\n",
    "# fitting and transforming the text data\n",
    "X = vectorizer.fit_transform(lyric_text)\n",
    "\n",
    "# checking shape\n",
    "print(X.shape)\n",
    "\n",
    "# getting feature names for further analysis\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# checking top features\n",
    "looking_at_top(X, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to experiment with the lenth of the n-grams, so I will also be creating a (2,2) TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4584, 49457)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>148.890813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me</td>\n",
       "      <td>73.944305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my</td>\n",
       "      <td>63.418646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh</td>\n",
       "      <td>59.750744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we</td>\n",
       "      <td>52.266900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>your</td>\n",
       "      <td>52.208628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all</td>\n",
       "      <td>47.852716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>46.305066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>know</td>\n",
       "      <td>42.288580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>so</td>\n",
       "      <td>37.412549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>when</td>\n",
       "      <td>36.507293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>just</td>\n",
       "      <td>33.784479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>never</td>\n",
       "      <td>33.284531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>love</td>\n",
       "      <td>32.986648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>oh oh</td>\n",
       "      <td>32.872976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>can</td>\n",
       "      <td>32.701313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>now</td>\n",
       "      <td>31.008053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>what</td>\n",
       "      <td>29.267935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>time</td>\n",
       "      <td>29.004960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>up</td>\n",
       "      <td>28.449700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cause</td>\n",
       "      <td>27.372446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1\n",
       "0     you  148.890813\n",
       "1      me   73.944305\n",
       "2      my   63.418646\n",
       "3      oh   59.750744\n",
       "4      we   52.266900\n",
       "5    your   52.208628\n",
       "6     all   47.852716\n",
       "7    like   46.305066\n",
       "8    know   42.288580\n",
       "9      so   37.412549\n",
       "10   when   36.507293\n",
       "11   just   33.784479\n",
       "12  never   33.284531\n",
       "13   love   32.986648\n",
       "14  oh oh   32.872976\n",
       "15    can   32.701313\n",
       "16    now   31.008053\n",
       "17   what   29.267935\n",
       "18   time   29.004960\n",
       "19     up   28.449700\n",
       "20  cause   27.372446"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents = 'ascii', stop_words = stop_words, ngram_range= (1, 3))\n",
    "\n",
    "# fitting\n",
    "X = vectorizer.fit_transform(lyric_text)\n",
    "\n",
    "# checking shape\n",
    "print(X.shape)\n",
    "\n",
    "# checking features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "looking_at_top(X, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4584, 49457)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>148.890813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>me</td>\n",
       "      <td>73.944305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my</td>\n",
       "      <td>63.418646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh</td>\n",
       "      <td>59.750744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we</td>\n",
       "      <td>52.266900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>your</td>\n",
       "      <td>52.208628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all</td>\n",
       "      <td>47.852716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>46.305066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>know</td>\n",
       "      <td>42.288580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>so</td>\n",
       "      <td>37.412549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>when</td>\n",
       "      <td>36.507293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>just</td>\n",
       "      <td>33.784479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>never</td>\n",
       "      <td>33.284531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>love</td>\n",
       "      <td>32.986648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>oh oh</td>\n",
       "      <td>32.872976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>can</td>\n",
       "      <td>32.701313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>now</td>\n",
       "      <td>31.008053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>what</td>\n",
       "      <td>29.267935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>time</td>\n",
       "      <td>29.004960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>up</td>\n",
       "      <td>28.449700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cause</td>\n",
       "      <td>27.372446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1\n",
       "0     you  148.890813\n",
       "1      me   73.944305\n",
       "2      my   63.418646\n",
       "3      oh   59.750744\n",
       "4      we   52.266900\n",
       "5    your   52.208628\n",
       "6     all   47.852716\n",
       "7    like   46.305066\n",
       "8    know   42.288580\n",
       "9      so   37.412549\n",
       "10   when   36.507293\n",
       "11   just   33.784479\n",
       "12  never   33.284531\n",
       "13   love   32.986648\n",
       "14  oh oh   32.872976\n",
       "15    can   32.701313\n",
       "16    now   31.008053\n",
       "17   what   29.267935\n",
       "18   time   29.004960\n",
       "19     up   28.449700\n",
       "20  cause   27.372446"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents = 'ascii', stop_words = stop_words, ngram_range= (1, 3))\n",
    "\n",
    "# fitting\n",
    "X_13 = vectorizer.fit_transform(lyric_text)\n",
    "\n",
    "# checking shape\n",
    "print(X_13.shape)\n",
    "\n",
    "# checking features\n",
    "feature_names_13 = vectorizer.get_feature_names_out()\n",
    "\n",
    "looking_at_top(X_13, feature_names_13)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
